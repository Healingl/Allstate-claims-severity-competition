{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported Spark Modules\n",
      "5\n",
      "defaultdict(<class 'int'>, {'scala': 1, 'hadoop': 1, 'java': 1, 'spark': 1, 'akka': 1})\n"
     ]
    }
   ],
   "source": [
    "#### This cell is to make spark work on a windows laptop\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Path for spark source folder\n",
    "os.environ['SPARK_HOME']=\"C:\\spark-2.0.1-bin-hadoop2.7\"\n",
    "\n",
    "# Append pyspark  to Python Path\n",
    "sys.path.append(\"C:\\spark-2.0.1-bin-hadoop2.7\\python\")\n",
    "sys.path.append(\"C:\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.3-src.zip\")\n",
    "#os.environ['SPARK_EXECUTOR_MEMORY']=\"5G\"\n",
    "\n",
    "try:\n",
    "    from pyspark import SparkContext\n",
    "    from pyspark import SparkConf\n",
    "    print (\"Successfully imported Spark Modules\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print (\"Can not import Spark Modules\", e)\n",
    "    sys.exit(1)\n",
    "\n",
    "# Initialize SparkContext\n",
    "sc = SparkContext()\n",
    "words = sc.parallelize([\"scala\",\"java\",\"hadoop\",\"spark\",\"akka\"])\n",
    "print (words.count())\n",
    "print(words.countByValue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from IPython.display import display\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql import SparkSession\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as func\n",
    "import matplotlib.patches as mpatches\n",
    "import time as time\n",
    "from matplotlib.patches import Rectangle\n",
    "import datetime\n",
    "import ast\n",
    "from operator import add\n",
    "import math\n",
    "from itertools import combinations\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import NaiveBayes\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import random\n",
    "import shelve\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "log4jLogger = sc._jvm.org.apache.log4j\n",
    "LOGGER = log4jLogger.LogManager.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First step: cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows before cleaning: 188319\n",
      "number of rows after cleaning: 188319\n",
      "Number of partitions: 2\n"
     ]
    }
   ],
   "source": [
    "input_path = \"train.csv\"\n",
    "raw_data = sc.textFile(input_path)\n",
    "print(\"number of rows before cleaning:\", raw_data.count())\n",
    "\n",
    "# extract the header\n",
    "header = raw_data.first()\n",
    "\n",
    "# replace invalid data with NULL and remove header\n",
    "cleaned_data = raw_data.filter(lambda row: row != header)\n",
    "\n",
    "print(\"number of rows after cleaning:\", raw_data.count())\n",
    "print(\"Number of partitions: \" + str(raw_data.getNumPartitions()))\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to put our data into a dataframe, so that it's going to be easier to plot graphs and get a better insight into our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'id;cat1;cat2;cat3;cat4;cat5;cat6;cat7;cat8;cat9;cat10;cat11;cat12;cat13;cat14;cat15;cat16;cat17;cat18;cat19;cat20;cat21;cat22;cat23;cat24;cat25;cat26;cat27;cat28;cat29;cat30;cat31;cat32;cat33;cat34;cat35;cat36;cat37;cat38;cat39;cat40;cat41;cat42;cat43;cat44;cat45;cat46;cat47;cat48;cat49;cat50;cat51;cat52;cat53;cat54;cat55;cat56;cat57;cat58;cat59;cat60;cat61;cat62;cat63;cat64;cat65;cat66;cat67;cat68;cat69;cat70;cat71;cat72;cat73;cat74;cat75;cat76;cat77;cat78;cat79;cat80;cat81;cat82;cat83;cat84;cat85;cat86;cat87;cat88;cat89;cat90;cat91;cat92;cat93;cat94;cat95;cat96;cat97;cat98;cat99;cat100;cat101;cat102;cat103;cat104;cat105;cat106;cat107;cat108;cat109;cat110;cat111;cat112;cat113;cat114;cat115;cat116;cont1;cont2;cont3;cont4;cont5;cont6;cont7;cont8;cont9;cont10;cont11;cont12;cont13;cont14;loss'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our taget is the loss. The id isn't actually useful and will be removed later. \"cat\" means categorical feature and \"cont\" means a continuous feature. we will use that to create the data schema. Note that we'll convert strings in Ints since it's easier to manipulate them later. (In particular for partitioning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = header.split(\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cats = names[1:117]\n",
    "conts = names[117:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tryeval(val,column_number):\n",
    "    if column_number == 0:\n",
    "        return int(val)\n",
    "    elif 1 <= column_number <= 116:\n",
    "        return val\n",
    "    elif 117 <= column_number <= 131:\n",
    "        return float(val)\n",
    "    else:\n",
    "        raise Exception(\"There is a big problem\")\n",
    "\n",
    "def to_tuple(string, character = \";\"):\n",
    "    list_of_strings = string.split(character)\n",
    "    return tuple(tryeval(string, n) for n, string in enumerate(list_of_strings))\n",
    "\n",
    "cleaned_data_splitted = cleaned_data.map(lambda x: to_tuple(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188318"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data_splitted.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll try to convert the categorical values in integers as it's way easier to work with later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_tuples(list_):\n",
    "    return tuple((string,) for string in list_)\n",
    "\n",
    "def fusion(x, y):\n",
    "    return tuple(tuple(set(xi + yi)) for xi, yi in zip(x,y))\n",
    "\n",
    "list_of_dictionaries = []\n",
    "a = cleaned_data_splitted.map(lambda x: to_tuples(x[1:117])).reduce(fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_tuples = tuple(tuple(sorted(tup)) for tup in a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the categories in order. We'll put them in a list of dictionaries as it makes it simpler to modify the RDD after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tup in sorted_tuples:\n",
    "    my_dict = dict()\n",
    "    for idx, cat in enumerate(tup):\n",
    "        my_dict[cat] = idx\n",
    "    list_of_dictionaries.append(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0, 'B': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_dictionaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bListOfDictionaries = sc.broadcast(list_of_dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace(row):\n",
    "    strings = row[1:117]\n",
    "    my_dicts = bListOfDictionaries.value\n",
    "    tuple_of_ints = ()\n",
    "    for dict_, string in zip(my_dicts, strings):\n",
    "        try:\n",
    "            tuple_of_ints += (dict_[string],)\n",
    "        except KeyError:\n",
    "            tuple_of_ints += (0,)\n",
    "    return (row[0],) + tuple_of_ints + row[117:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_rdd = cleaned_data_splitted.map(replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we now have integers instead of \"A\" or \"C\" or any other string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 1, 1, 3, 3, 1, 3, 2, 1, 3, 1, 0, 0, 0, 0, 0, 3, 1, 2, 4, 0, 2, 15, 1, 6, 0, 0, 8, 4, 6, 9, 6, 45, 28, 2, 19, 55, 0, 14, 269, 0.7263, 0.245921, 0.187583, 0.789639, 0.310061, 0.718367, 0.33506, 0.3026, 0.67135, 0.8351, 0.569745, 0.594646, 0.822493, 0.714843, 2213.18)\n"
     ]
    }
   ],
   "source": [
    "print(final_rdd.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_delete = [2,3,4,5,6,7,8,96]    # cela correspond à cat2, cat3, cat4 ...\n",
    "features_to_keep = list(range(116))\n",
    "for idx in to_delete:\n",
    "    features_to_keep.remove(idx - 1) # Car to_delete commence à 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame(final_rdd.map(lambda x: (float(x[-1]), Vectors.dense(x[1:-1]))), [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=20).fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(train, test) = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(labelCol=\"label\", featuresCol=\"indexedFeatures\", maxDepth=20, maxBins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[indexer, dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictionsTrain = model.transform(train)\n",
    "predictionsTest = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on test data = 631.863\n",
      "Mean Squared Error (MSE) on test data = 1570.83\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(metricName=\"mae\")\n",
    "rmseTrain = evaluator.evaluate(predictionsTrain)\n",
    "print(\"Mean Squared Error (MSE) on test data = %g\" % rmseTrain)\n",
    "\n",
    "rmseTest = evaluator.evaluate(predictionsTest)\n",
    "print(\"Mean Squared Error (MSE) on test data = %g\" % rmseTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining the transformations\n",
    "slicer = VectorSlicer(inputCol=\"features\", outputCol=\"featuresSliced\", indices = features_to_keep)\n",
    "indexer = VectorIndexer(inputCol=\"featuresSliced\", outputCol=\"indexedFeatures\", maxCategories=20).fit(slicer.transform(df))\n",
    "dt = DecisionTreeRegressor(labelCol=\"label\", featuresCol=\"indexedFeatures\", maxBins=20, minInstancesPerNode=1, minInfoGain=0.0, maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, impurity=\"variance\")\n",
    "\n",
    "# defining the pipeline\n",
    "pipeline = Pipeline(stages=[slicer, indexer, dt])\n",
    "\n",
    "# defining the parameters to test\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [5]) \\\n",
    "    .build()\n",
    "    \n",
    "myRegressor = RegressionEvaluator(metricName=\"mae\")\n",
    "    \n",
    "# defining the cross-validation\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=myRegressor,\n",
    "                          numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation results (we see that the depth of 10 is better):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1433.5199130594435]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_4377bc1d5dd4a225144b"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training error for the best model (we're overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1424.2979358651687"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RegressionEvaluator(metricName=\"mae\").evaluate(cvModel.bestModel.transform(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try with all the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=20).fit(df)\n",
    "dt = DecisionTreeRegressor(labelCol=\"label\", featuresCol=\"indexedFeatures\", maxBins=20)\n",
    "\n",
    "# defining the pipeline\n",
    "pipeline = Pipeline(stages=[slicer, indexer, dt])\n",
    "\n",
    "# defining the parameters to test\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [10, 20]) \\\n",
    "    .build()\n",
    "    \n",
    "# defining the cross-validation\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(metricName=\"mae\"),\n",
    "                          numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it better than last time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1359.6923954368876, 1588.2854298996053]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that those features bring absolutely no information. As shown in the other notebook \"Allstate competition.ipynb\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|  label|            features|      scaledFeatures|\n",
      "+-------+--------------------+--------------------+\n",
      "|2213.18|[0.0,1.0,0.0,1.0,...|[0.0,1.0,0.0,1.0,...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "None\n",
      "[1331.6055414944035]\n",
      "1329.4492453430448\n",
      "[MinMaxScaler_4aeb955f79fe5cfac729, LinearRegression_48a4bb18db05c6db6cc5]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "# Defining the transformations\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\").fit(df)\n",
    "\n",
    "print(scaler.transform(df).show(1))\n",
    "dt = LinearRegression(featuresCol=\"scaledFeatures\")\n",
    "\n",
    "# defining the pipeline\n",
    "pipeline = Pipeline(stages=[scaler,dt])\n",
    "\n",
    "# defining the parameters to test\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxIter, [25]) \\\n",
    "    .build()\n",
    "    \n",
    "myEvaluator = RegressionEvaluator(metricName=\"mae\")\n",
    "# defining the cross-validation\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=myEvaluator,\n",
    "                          numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(df)\n",
    "\n",
    "print(cvModel.avgMetrics)\n",
    "print(myEvaluator.evaluate(cvModel.bestModel.transform(df)))\n",
    "print(cvModel.bestModel.stages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame(final_rdd.map(lambda x: (float(x[-1]), Vectors.dense(x[1:117]), Vectors.dense(x[117:-1]))), [\"label\", \"cat_features\", \"cont_features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|  label|        cat_features|       cont_features|\n",
      "+-------+--------------------+--------------------+\n",
      "|2213.18|[0.0,1.0,0.0,1.0,...|[0.7263,0.245921,...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try different configurations for a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining the transformations\n",
    "indexer = VectorIndexer(inputCol=\"cat_features\", outputCol=\"indexedFeatures\", maxCategories=20).fit(df)\n",
    "slicer_cat = VectorSlicer(inputCol=\"indexedFeatures\", outputCol=\"sliced_cat_Features\", indices=features_to_keep)\n",
    "slicer_cont = VectorSlicer(inputCol=\"cont_features\", outputCol=\"sliced_cont_Features\", indices = list(range(1)))\n",
    "assembler = VectorAssembler(inputCols=[\"sliced_cat_Features\", \"sliced_cont_Features\"], outputCol=\"features\")\n",
    "\n",
    "rf = RandomForestRegressor(labelCol=\"label\", featuresCol=\"features\", maxBins=280,\\\n",
    "                           maxMemoryInMB=500, subsamplingRate=0.9, cacheNodeIds=True, \n",
    "                           checkpointInterval=10, numTrees=5)\n",
    "\n",
    "# defining the pipeline\n",
    "pipeline = Pipeline(stages=[indexer, slicer_cat, slicer_cont, assembler, rf])\n",
    "\n",
    "# defining the parameters to test\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [3]) \\\n",
    "    .build()\n",
    "    \n",
    "myRegressor = RegressionEvaluator(metricName=\"mae\")\n",
    "    \n",
    "# defining the cross-validation\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=myRegressor,\n",
    "                          numFolds=3)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1506.7323171092319]\n",
      "1505.6181538122055\n",
      "[VectorIndexer_4557be16f2cf9e01e187, VectorSlicer_4ee8b70f28ee273d0226, VectorSlicer_4740827b3bfdca001282, VectorAssembler_4a18a2d0732f223883b7, RandomForestRegressionModel (uid=rfr_a9b44af90c4d) with 5 trees]\n"
     ]
    }
   ],
   "source": [
    "print(cvModel.avgMetrics)\n",
    "print(myRegressor.evaluate(cvModel.bestModel.transform(df)))\n",
    "print(cvModel.bestModel.stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label', 'cat_features', 'cont_features']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(label=2213.18, cat_features=DenseVector([0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 3.0, 1.0, 1.0, 3.0, 3.0, 1.0, 3.0, 2.0, 1.0, 3.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 1.0, 2.0, 4.0, 0.0, 2.0, 15.0, 1.0, 6.0, 0.0, 0.0, 8.0, 4.0, 6.0, 9.0, 6.0, 45.0, 28.0, 2.0, 19.0, 55.0, 0.0, 14.0, 269.0]), cont_features=DenseVector([0.7263, 0.2459, 0.1876, 0.7896, 0.3101, 0.7184, 0.3351, 0.3026, 0.6714, 0.8351, 0.5697, 0.5946, 0.8225, 0.7148]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def t(element, fitInfo = 0, args = []):\n",
    "        new_tup = tuple(int(scalar*args[0]) for scalar in element)\n",
    "        return Vectors.dense(new_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class customTransformer:\n",
    "    \n",
    "    def __init__(self, inputCol, outputCol, *others):\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "        self.args = list(others)\n",
    "        self.fitInfo = 0\n",
    "        \n",
    "    # Store information taken from the dataframe\n",
    "    def fit(self, df, fitting):\n",
    "        idx = df.columns.index(self.inputCol)\n",
    "        self.fitInfo = fitting(df.rdd.map(lambda x: x[idx]))\n",
    "        return self\n",
    "            \n",
    "    # This transforms a dataframe into another dataframe\n",
    "    def transform(self, df, transforming):\n",
    "        \n",
    "        # We get the index of the colmumns we'll be working on\n",
    "        names = df.columns\n",
    "        idx = names.index(self.inputCol)\n",
    "        \n",
    "        # We apply the transformation\n",
    "        bInfo = sc.broadcast(tuple([0,[10]]))\n",
    "        new_column = df.rdd.map(lambda x: transforming(x[idx], bInfo.value[0], bInfo.value[1]), True)\n",
    "        \n",
    "        # We attach the results to the old rdd\n",
    "        old_rdd = df.rdd.map(lambda x: list(x))\n",
    "        new_rdd = old_rdd.zip(new_column).map(lambda x: x[0] + [x[1]])\n",
    "        new_names = names + [self.outputCol]\n",
    "        \n",
    "        return sqlContext.createDataFrame(new_rdd, new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply(df, listOfTransformers):\n",
    "    df1 = df\n",
    "    for transformer in listOfTransformers:\n",
    "        df1 = transformer.transform(df1)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def t_bucketize(element, fitInfo = 0, args = []):\n",
    "    new_tup = tuple(int(scalar*args[0]) for scalar in element)\n",
    "    return Vectors.dense(new_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-process\n",
    "df1 = customTransformer(\"cont_features\", \"cont_bucked_features\", 7).transform(df, t_bucketize).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining the transformations\n",
    "indexer_cat = VectorIndexer(inputCol=\"cat_features\", outputCol=\"cat_indexedFeatures\", maxCategories=20).fit(df1)\n",
    "indexer_cont = VectorIndexer(inputCol=\"cont_bucked_features\", outputCol=\"cont_indexedFeatures\", maxCategories=7).fit(df1)\n",
    "slicer_cat = VectorSlicer(inputCol=\"cat_indexedFeatures\", outputCol=\"sliced_cat_Features\", indices=features_to_keep)\n",
    "slicer_cont = VectorSlicer(inputCol=\"cont_indexedFeatures\", outputCol=\"sliced_cont_Features\", indices = list(range(2)))\n",
    "assembler = VectorAssembler(inputCols=[\"sliced_cat_Features\", \"sliced_cont_Features\"], outputCol=\"features\")\n",
    "\n",
    "rf = RandomForestRegressor(labelCol=\"label\", featuresCol=\"features\", maxBins=300,\\\n",
    "                           maxMemoryInMB=500, subsamplingRate=0.9, cacheNodeIds=True, \n",
    "                           checkpointInterval=10, numTrees=5)\n",
    "\n",
    "# defining the pipeline\n",
    "pipeline = Pipeline(stages=[indexer_cat, indexer_cont, slicer_cat, slicer_cont, assembler, rf])\n",
    "\n",
    "# defining the parameters to test\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [3]) \\\n",
    "    .build()\n",
    "    \n",
    "myRegressor = RegressionEvaluator(metricName=\"mae\")\n",
    "    \n",
    "# defining the cross-validation\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=myRegressor,\n",
    "                          numFolds=3)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1502.4312263494066]\n",
      "1496.8333043057692\n",
      "[VectorIndexer_426baa11bfc634256996, VectorIndexer_44a39d01fe167cb77622, VectorSlicer_4ff28b35d924d51b6a10, VectorSlicer_4f58872cab1f2ec64485, VectorAssembler_4ab7a49df3fc35f97d31, RandomForestRegressionModel (uid=rfr_734c16e8a26e) with 5 trees]\n"
     ]
    }
   ],
   "source": [
    "print(cvModel.avgMetrics)\n",
    "print(myRegressor.evaluate(cvModel.bestModel.transform(df1)))\n",
    "print(cvModel.bestModel.stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation of spark isn't really helpful because we can't get the results are they are calculated. We'll do a custom search for hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the transformations\n",
    "indexer_cat = VectorIndexer(inputCol=\"cat_features\", outputCol=\"cat_indexedFeatures\", maxCategories=300).fit(df1)\n",
    "indexer_cont = VectorIndexer(inputCol=\"cont_bucked_features\", outputCol=\"cont_indexedFeatures\", maxCategories=7).fit(df1)\n",
    "slicer_cat = VectorSlicer(inputCol=\"cat_indexedFeatures\", outputCol=\"sliced_cat_Features\", indices=features_to_keep)\n",
    "slicer_cont = VectorSlicer(inputCol=\"cont_indexedFeatures\", outputCol=\"sliced_cont_Features\", indices = list(range(2)))\n",
    "assembler = VectorAssembler(inputCols=[\"sliced_cat_Features\", \"sliced_cont_Features\"], outputCol=\"features\")\n",
    "\n",
    "rf = RandomForestRegressor(labelCol=\"label\", featuresCol=\"features\", maxBins=300,\\\n",
    "                           maxMemoryInMB=500, subsamplingRate=0.9, cacheNodeIds=True, \n",
    "                           checkpointInterval=10, numTrees=5)\n",
    "\n",
    "df2 = apply(df1,[indexer_cat, indexer_cont, slicer_cat, slicer_cont, assembler]).cache()\n",
    "df1.unpersist()\n",
    "# defining the pipeline\n",
    "pipeline = Pipeline(stages=[rf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "depths = [3,5,7,10,14,17,20,25]\n",
    "\n",
    "for depth in depths:\n",
    "\n",
    "    # defining the parameters to test\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(rf.maxDepth, [depth]) \\\n",
    "        .build()\n",
    "\n",
    "    myRegressor = RegressionEvaluator(metricName=\"mae\")\n",
    "\n",
    "    # defining the cross-validation\n",
    "    crossval = CrossValidator(estimator=pipeline,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=myRegressor,\n",
    "                              numFolds=3)  # use 3+ folds in practice\n",
    "\n",
    "    # Run cross-validation, and choose the best set of parameters.\n",
    "    cvModel = crossval.fit(df2)\n",
    "    print(\"depth:\" + str(depth))\n",
    "    print(cvModel.avgMetrics[0]/3)\n",
    "    print(myRegressor.evaluate(cvModel.bestModel.transform(df2)))\n",
    "    print(cvModel.bestModel.stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try different sets of features to have an idea of which ones are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We now have 10 set of features to test\n",
    "sets_cat_features = [sorted(random.sample(features_to_keep,60)) for _ in range(10)]\n",
    "sets_cont_features = [sorted(random.sample(list(range(14)),8)) for _ in range(10)]\n",
    "sets_features = list(zip(sets_cat_features,sets_cont_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the transformations\n",
    "indexer_cat = VectorIndexer(inputCol=\"cat_features\", outputCol=\"cat_indexedFeatures\", maxCategories=300).fit(df1)\n",
    "indexer_cont = VectorIndexer(inputCol=\"cont_bucked_features\", outputCol=\"cont_indexedFeatures\", maxCategories=7).fit(df1)\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor(labelCol=\"label\", featuresCol=\"features\", maxBins=300,\\\n",
    "                           maxMemoryInMB=500, subsamplingRate=0.9, cacheNodeIds=True, \n",
    "                           checkpointInterval=10, maxDepth=9, numTrees=5)\n",
    "\n",
    "df2 = apply(df1,[indexer_cat, indexer_cont]).cache()\n",
    "df1.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for set_features in sets_features:\n",
    "\n",
    "    slicer_cat=VectorSlicer(inputCol=\"cat_indexedFeatures\", outputCol=\"sliced_cat_Features\", indices=set_features[0])\n",
    "    slicer_cont=VectorSlicer(inputCol=\"cont_indexedFeatures\", outputCol=\"sliced_cont_Features\", indices = set_features[1])\n",
    "    assembler = VectorAssembler(inputCols=[\"sliced_cat_Features\", \"sliced_cont_Features\"], outputCol=\"features\")\n",
    "    pipeline = Pipeline(stages=[slicer_cat, slicer_cont, assembler,rf])\n",
    "    # defining the parameters to test\n",
    "    paramGrid = ParamGridBuilder()\\\n",
    "        .build()\n",
    "\n",
    "    myRegressor = RegressionEvaluator(metricName=\"mae\")\n",
    "\n",
    "    # defining the cross-validation\n",
    "    crossval = CrossValidator(estimator=pipeline,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=myRegressor,\n",
    "                              numFolds=3)  # use 3+ folds in practice\n",
    "\n",
    "    # Run cross-validation, and choose the best set of parameters.\n",
    "    cvModel = crossval.fit(df2)\n",
    "    print(\"features: \" + str(set_features))\n",
    "    print(cvModel.avgMetrics[0]/3) # this is because of a bug of spark 2.0.0\n",
    "    print(myRegressor.evaluate(cvModel.bestModel.transform(df2)))\n",
    "    print(cvModel.bestModel.stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may be crazy, but i'll try to fit a linear regression to the result of the cross-validation. Reading the weights should tell us what features to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_results = []\n",
    "cv_results.append(tuple([[0, 8, 20, 23, 29, 30, 33, 34, 35, 37, 42, 43, 44, 45, 46, 47, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 63, 64, 66, 67, 68, 69, 70, 72, 73, 74, 75, 78, 81, 82, 83, 84, 85, 88, 89, 91, 92, 93, 96, 98, 100, 101, 103, 106, 107, 108, 109, 112, 114, 115], [0, 1, 2, 5, 7, 9, 11, 13],1431.54795088]))\n",
    "cv_results.append(tuple([[0, 11, 12, 13, 15, 18, 20, 23, 25, 26, 28, 29, 31, 33, 35, 36, 37, 38, 40, 41, 43, 45, 46, 47, 48, 49, 50, 53, 54, 57, 59, 60, 65, 67, 72, 75, 77, 79, 81, 82, 83, 84, 85, 87, 88, 89, 91, 92, 93, 96, 97, 98, 99, 100, 101, 102, 104, 108, 112, 113], [0, 1, 2, 3, 7, 8, 10, 11],1372.77685625]))\n",
    "cv_results.append(tuple([[8, 9, 12, 15, 16, 18, 19, 21, 22, 24, 29, 31, 34, 37, 39, 41, 42, 43, 44, 45, 47, 48, 52, 53, 55, 57, 58, 59, 61, 63, 65, 71, 73, 75, 76, 78, 79, 80, 81, 82, 84, 87, 89, 91, 93, 94, 96, 97, 98, 99, 102, 103, 104, 106, 107, 108, 109, 112, 113, 114], [0, 1, 2, 3, 4, 5, 8, 12],1382.60758841]))\n",
    "cv_results.append(tuple([[8, 12, 14, 17, 18, 20, 21, 22, 23, 27, 28, 30, 32, 34, 35, 36, 38, 39, 40, 42, 45, 47, 50, 52, 53, 54, 55, 56, 57, 58, 59, 62, 63, 67, 68, 69, 72, 75, 76, 82, 83, 84, 86, 87, 88, 89, 90, 91, 93, 94, 97, 98, 99, 100, 102, 103, 104, 106, 110, 111], [2, 3, 4, 6, 8, 10, 11, 13],1453.34577263]))\n",
    "cv_results.append(tuple([[8, 10, 13, 14, 15, 16, 17, 18, 19, 22, 23, 28, 30, 34, 35, 40, 42, 43, 47, 49, 50, 51, 53, 54, 57, 61, 62, 63, 64, 67, 68, 69, 70, 71, 72, 77, 79, 81, 82, 83, 84, 86, 87, 88, 89, 90, 93, 94, 96, 98, 101, 103, 104, 107, 108, 109, 110, 112, 114, 115], [1, 2, 3, 5, 6, 9, 10, 13],1435.7798927]))\n",
    "cv_results.append(tuple([[10, 12, 13, 14, 17, 19, 20, 22, 23, 26, 32, 33, 34, 36, 37, 38, 39, 40, 43, 47, 50, 54, 55, 57, 60, 61, 63, 64, 65, 66, 67, 69, 70, 72, 74, 76, 77, 78, 80, 81, 82, 84, 85, 86, 87, 89, 90, 92, 93, 97, 99, 100, 102, 107, 109, 110, 111, 112, 113, 115], [0, 2, 3, 5, 6, 8, 10, 12],1406.99266046]))\n",
    "cv_results.append(tuple([[0, 8, 12, 17, 19, 20, 21, 25, 26, 27, 31, 32, 33, 34, 35, 36, 39, 40, 41, 44, 45, 47, 49, 52, 54, 56, 58, 59, 62, 63, 64, 65, 67, 68, 70, 71, 72, 75, 76, 78, 80, 82, 84, 85, 86, 87, 88, 92, 93, 94, 97, 98, 101, 102, 105, 107, 108, 112, 113, 114], [0, 2, 5, 6, 8, 9, 10, 13],1427.41651413]))\n",
    "cv_results.append(tuple([[0, 9, 11, 14, 16, 17, 18, 20, 22, 23, 24, 28, 30, 31, 34, 35, 36, 37, 39, 40, 45, 49, 50, 51, 52, 53, 54, 58, 59, 61, 64, 65, 67, 68, 70, 71, 73, 74, 76, 79, 80, 81, 83, 84, 87, 88, 90, 91, 94, 98, 99, 102, 103, 105, 106, 107, 109, 110, 112, 114], [0, 1, 5, 7, 8, 9, 10, 11],1369.85793953]))\n",
    "cv_results.append(tuple([[8, 12, 14, 15, 17, 18, 23, 26, 27, 30, 31, 36, 37, 39, 40, 43, 44, 45, 46, 47, 49, 50, 51, 54, 55, 56, 58, 61, 62, 63, 65, 66, 71, 72, 74, 77, 78, 80, 81, 85, 86, 87, 89, 90, 91, 93, 96, 97, 98, 99, 100, 101, 104, 105, 106, 107, 108, 109, 112, 115], [0, 3, 4, 8, 9, 10, 12, 13],1405.87042128]))\n",
    "cv_results.append(tuple([[0, 9, 14, 15, 17, 19, 21, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 36, 38, 39, 40, 41, 44, 47, 48, 50, 51, 52, 54, 55, 56, 58, 59, 60, 62, 63, 65, 66, 68, 69, 71, 73, 74, 75, 76, 80, 84, 85, 89, 93, 94, 97, 100, 102, 105, 107, 109, 110, 114, 115], [1, 2, 4, 6, 9, 11, 12, 13],1464.52998139]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_binary(my_list,n):\n",
    "    result = [0 for _ in range(n)]\n",
    "    for i in my_list:\n",
    "        result[i] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for result in cv_results:\n",
    "    xi = to_binary(result[0],116) + to_binary(result[1],14)\n",
    "    x.append(xi)\n",
    "    \n",
    "    y.append(result[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [37, 116, 99, 124, 81, 11, 65, 79, 112, 61, 31, 113, 123, 91, 87, 49, 45, 18, 80, 16, 12, 126, 43, 121, 102, 20, 77, 9, 24, 106, 98, 22, 53, 46, 78, 96, 92, 13, 36, 119, 117, 48, 108, 0, 73, 74, 39, 64, 70, 90, 109, 67, 105, 41, 26, 128, 71, 104, 60, 107, 59, 76, 83, 34, 1, 4, 6, 5, 2, 95, 3, 7, 15, 29, 97, 40, 103, 35, 88, 57, 84, 101, 10, 85, 127, 82, 23, 50, 25, 58, 111, 33, 114, 28, 54, 51, 72, 38, 52, 14, 27, 44, 66, 125, 19, 17, 94, 100, 110, 89, 120, 86, 75, 55, 47, 93, 8, 42, 118, 21, 115, 30, 32, 63, 68, 56, 122, 62, 69, 129]\n"
     ]
    }
   ],
   "source": [
    "print('Coefficients: \\n', sorted(range(len(regr.coef_)), key=lambda k: regr.coef_[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "given a list, we should be able to search the best combinaisons of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat_cat = [37, 116, 99, 124, 81, 11, 65, 79, 112, 61, 31, 113, 123, 91, 87, 49, 45, 18, 80, 16, 12, 126, 43, 121, 102, 20, 77, 9, 24, 106, 98, 22, 53, 46, 78, 96, 92, 13, 36, 119, 117, 48, 108, 0, 73, 74, 39, 64, 70, 90, 109, 67, 105, 41, 26, 128, 71, 104, 60, 107, 59, 76, 83, 34, 1, 4, 6, 5, 2, 95, 3, 7, 15, 29, 97, 40, 103, 35, 88, 57, 84, 101, 10, 85, 127, 82, 23, 50, 25, 58, 111, 33, 114, 28, 54, 51, 72, 38]\n",
    "feat_cont = [a - 116  for a in feat_cat if a>=116]\n",
    "feat_cat = [a for a in feat_cat if a<116]\n",
    "# We now have 10 set of features to test\n",
    "sets_cat_features = [sorted(random.sample(feat_cat,60)) for _ in range(10)]\n",
    "sets_cont_features = [sorted(random.sample(feat_cont,8)) for _ in range(10)]\n",
    "sets_features = list(zip(sets_cat_features,sets_cont_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's try to parse the results, it'll be easier to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "f = open(\"results_cluster.txt\",\"r\")\n",
    "for i,line in enumerate(f):\n",
    "    if line not in ['\\n', '\\r\\n']:\n",
    "        if i%4 == 0:\n",
    "            g = line[12:-3]\n",
    "            LOGGER.warn(g)\n",
    "            a, b = g.split(\"]\")\n",
    "            b = b[3:]\n",
    "            a = [int(j) for j in a.split(\",\")]\n",
    "            b = [int(j) for j in b.split(\",\")]\n",
    "            x.append(to_binary(a,116) + to_binary(b,14))\n",
    "        if i%4 ==1:\n",
    "            y.append(float(line[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [118, 79, 116, 14, 23, 61, 6, 57, 109, 59, 121, 78, 54, 99, 68, 47, 119, 117, 15, 49, 45, 124, 126, 97, 0, 127, 76, 129, 80, 81, 25, 114, 36, 92, 91, 58, 103, 104, 18, 65, 113, 128, 7, 27, 100, 101, 82, 20, 106, 53, 122, 13, 72, 105, 40, 9, 108, 115, 35, 63, 17, 83, 66, 93, 52, 24, 60, 8, 1, 75, 102, 39, 84, 28, 51, 85, 43, 86, 33, 44, 34, 70, 21, 95, 87, 16, 3, 69, 74, 123, 71, 26, 56, 94, 2, 29, 73, 38, 30, 77, 11, 19, 37, 64, 90, 12, 46, 50, 41, 112, 31, 10, 107, 48, 111, 120, 55, 22, 62, 88, 42, 4, 125, 96, 98, 32, 5, 110, 67, 89]\n"
     ]
    }
   ],
   "source": [
    "print('Coefficients: \\n', sorted(range(len(regr.coef_)), key=lambda k: regr.coef_[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I was at 14 trees with 14 of depth<br>\n",
    "I should first try with 5 trees of depth 14 and look at the result.<br>\n",
    "Then I should use only a 5% sample of the dataset to iterate quickly. look at the result.<br>\n",
    "Then I should perform a feature selection. I'll select at random 110 features and look at the result of a cross-validation.<br>\n",
    "after that, I should select 105 features from the 120 features that I previously selected.\n",
    "then 100, 95, and so on and so on until it's impossible to improve the score.\n",
    "Then I should use the full dataset and do a hyperparameter tuning for the depth of the trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rand_forest_train(df2, depth, nb_trees, features=None, sample_data=100,verbose = True, test_set = None):\n",
    "    \n",
    "    if features is None:\n",
    "        features = features_to_keep + list(range(116,130))\n",
    "    \n",
    "    # Transforming features:\n",
    "    feat_cont = [a - 116  for a in features if a>=116]\n",
    "    feat_cat = [a for a in features if a<116]\n",
    "        \n",
    "    \n",
    "    # Sample data\n",
    "    if sample_data == 100:\n",
    "        df3 = df2.cache()\n",
    "    else:\n",
    "        df3 = df2.sample(False,sample_data*0.01, 7).cache()\n",
    "    \n",
    "    slicer_cat = VectorSlicer(inputCol=\"cat_indexedFeatures\", outputCol=\"sliced_cat_Features\", indices=feat_cat)\n",
    "    slicer_cont = VectorSlicer(inputCol=\"cont_indexedFeatures\", outputCol=\"sliced_cont_Features\", indices = feat_cont)\n",
    "    assembler = VectorAssembler(inputCols=[\"sliced_cat_Features\", \"sliced_cont_Features\"], outputCol=\"features\")\n",
    "    \n",
    "    # This is to avoid any problem with the sclicers not having at least one index\n",
    "    if feat_cont != []:\n",
    "        \n",
    "        rf = RandomForestRegressor(labelCol=\"label\", featuresCol=\"features\", maxBins=300,subsamplingRate=0.9)\n",
    "        # defining the pipeline\n",
    "        pipeline = Pipeline(stages=[slicer_cat, slicer_cont, assembler, rf])\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        rf = RandomForestRegressor(labelCol=\"label\", featuresCol=\"sliced_cat_Features\", maxBins=300,subsamplingRate=0.9)\n",
    "        # defining the pipeline\n",
    "        pipeline = Pipeline(stages=[slicer_cat, rf])\n",
    "        \n",
    "        \n",
    "    # defining the parameters to test\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(rf.maxDepth, [depth]) \\\n",
    "        .addGrid(rf.numTrees, [nb_trees]) \\\n",
    "        .build()\n",
    "\n",
    "    myRegressor = RegressionEvaluator(metricName=\"mae\")\n",
    "\n",
    "    # defining the cross-validation\n",
    "    crossval = CrossValidator(estimator=pipeline,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=myRegressor,\n",
    "                              numFolds=3)  # use 3+ folds in practice\n",
    "\n",
    "    # Run cross-validation, and choose the best set of parameters.\n",
    "    cvModel = crossval.fit(df3)\n",
    "    cv_err = cvModel.avgMetrics[0]\n",
    "    train_err = myRegressor.evaluate(cvModel.bestModel.transform(df3))\n",
    "    df3.unpersist()\n",
    "    if verbose:\n",
    "        print(\"depth: \" + str(depth))\n",
    "        # cv_err = cvModel.avgMetrics[0]/3\n",
    "        print(cv_err)\n",
    "        print(train_err)\n",
    "        print(cvModel.bestModel.stages)\n",
    "    if test_set is None:\n",
    "        return cv_err, train_err\n",
    "    else:\n",
    "        return cvModel.bestModel.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, cat_features: vector, cont_features: vector, cont_bucked_features: vector]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = customTransformer(\"cont_features\", \"cont_bucked_features\", 7).transform(df, t_bucketize).cache()\n",
    "indexer_cat = VectorIndexer(inputCol=\"cat_features\", outputCol=\"cat_indexedFeatures\", maxCategories=300).fit(df1)\n",
    "indexer_cont = VectorIndexer(inputCol=\"cont_bucked_features\", outputCol=\"cont_indexedFeatures\", maxCategories=7).fit(df1)\n",
    "\n",
    "df2 = apply(df1,[indexer_cat, indexer_cont]).cache()\n",
    "df1.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth: 14\n",
      "1388.224360138156\n",
      "1157.8044616016518\n",
      "[VectorSlicer_4d7ab582670219e88461, RandomForestRegressionModel (uid=rfr_dd1bc6d94b52) with 4 trees]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1388.224360138156, 1157.8044616016518)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_forest_train(df2,14,4 ,features_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth: 14\n",
      "1391.2518025022043\n",
      "1159.427496966247\n",
      "[VectorSlicer_4e1683990c6dcb21986e, RandomForestRegressionModel (uid=rfr_3dc623f694d6) with 4 trees]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1391.2518025022043, 1159.427496966247)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_forest_train(df2,14,4 ,features_to_keep[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth: 14\n",
      "1362.3767349065674\n",
      "1136.6829410115051\n",
      "[VectorSlicer_498cb439ac0053411ffe, VectorSlicer_4537ad7083b5d891242e, VectorAssembler_4654a6ede05a0b0af53b, RandomForestRegressionModel (uid=rfr_8dbc6c18ef39) with 4 trees]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1362.3767349065674, 1136.6829410115051)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_forest_train(df2,14,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth: 14\n",
      "1363.5000239883748\n",
      "1125.9962479647313\n",
      "[VectorSlicer_4f8788e46d9bfca6a941, VectorSlicer_49899d1073a5a95c03ea, VectorAssembler_4e3394217dc5f5589911, RandomForestRegressionModel (uid=rfr_d555fe3b28f2) with 4 trees]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1363.5000239883748, 1125.9962479647313)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_forest_train(df2,14,4, list(range(130)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth: 14\n",
      "1321.398871930481\n",
      "1156.5779933130573\n",
      "[VectorSlicer_406bbe9e42e724cf6966, VectorSlicer_4ea9b01df8045cd9c804, VectorAssembler_49449f48a0f6bc9aa461, RandomForestRegressionModel (uid=rfr_400859350e36) with 4 trees]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1321.398871930481, 1156.5779933130573)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_forest_train(df2,14,4 ,features_to_keep[:-10]  + list(range(116,130)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth: 14\n",
      "1319.4558780541247\n",
      "1159.1153442066277\n",
      "[VectorSlicer_4fd696c6e299c95bf0d0, VectorSlicer_41d48e9a7de05ab729c5, VectorAssembler_4f1198516e7cb6ba7d88, RandomForestRegressionModel (uid=rfr_9aab3bb63ba6) with 4 trees]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1319.4558780541247, 1159.1153442066277)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_forest_train(df2,14,4 ,features_to_keep[:-10] + [115]  + list(range(116,130)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_random(list_, nb_delete = 5):\n",
    "    l = len(list_)\n",
    "    nb_keep = l-nb_delete\n",
    "    return sorted(random.sample(list_,nb_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_test = [delete_random(features_to_keep + list(range(116,130)),5) for _ in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for features in to_test:\n",
    "    results.append(rand_forest_train(df2,14,4, features, verbose=False))\n",
    "    LOGGER.warn(str(results[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic = shelve.open(\"database_results_laptop\",writeback = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.seed(72)\n",
    "while True:\n",
    "    features = delete_random(features_to_keep + list(range(116,130)),5)\n",
    "    cv_err,train_err = rand_forest_train(df2,14,4, features, verbose=False)\n",
    "    LOGGER.warn(str(features))\n",
    "    LOGGER.warn(str(cv_err))\n",
    "    LOGGER.warn(str(train_err))\n",
    "    dic[\"depth 14 trees 4 features 117\"].append(tuple([features, cv_err, train_err]))\n",
    "    dic.sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for tup in dic[\"depth 14 trees 4 features 117\"]:\n",
    "    xi = to_binary(tup[0],130)\n",
    "    x.append(xi)\n",
    "    \n",
    "    y.append(tup[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic1 = shelve.open(\"database_results\",writeback = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tup in dic1[\"depth 14 trees 4 features 117\"]:\n",
    "    xi = to_binary(tup[0],130)\n",
    "    x.append(xi)\n",
    "    y.append(tup[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine(dic, dic1, key):\n",
    "    x = []\n",
    "    y = []\n",
    "    for tup in dic[key]:\n",
    "        xi = to_binary(tup[0],130)\n",
    "        x.append(xi)\n",
    "        y.append(tup[1])\n",
    "    \n",
    "    for tup in dic1[key]:\n",
    "        xi = to_binary(tup[0],130)\n",
    "        x.append(xi)\n",
    "        y.append(tup[1])\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104.21080683908593"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "regr = linear_model.Ridge()\n",
    "#regr.fit(x[:-10],y[:-10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.457232282795381"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "-np.mean(cross_val_score(regr, x, y, scoring='neg_mean_squared_error', cv = len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.39002056e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,  -2.07757038e+00,\n",
       "        -2.08164497e+00,  -2.93319715e+00,  -5.18061378e+00,\n",
       "        -3.28234594e-01,  -2.43288320e+00,   5.13360082e-01,\n",
       "        -1.48666719e+00,   1.02705003e+00,   1.37958577e-01,\n",
       "        -2.45857321e-01,  -8.53273567e-01,   7.45265374e-01,\n",
       "        -2.68345392e-01,  -4.24673871e-01,  -1.32173759e+00,\n",
       "        -2.03203357e+00,  -5.73600870e-01,  -2.04767275e+00,\n",
       "        -8.96189954e-01,   1.88327766e+00,   1.74977196e+00,\n",
       "         1.69861234e+00,   3.54708141e-01,  -9.51013283e-01,\n",
       "         9.64999508e-01,   2.38708976e+00,  -1.10236469e+00,\n",
       "        -6.02209645e-01,  -1.76324668e+00,   9.46917609e-01,\n",
       "         1.08995379e-01,   8.47604966e-01,  -1.15452829e-01,\n",
       "         6.15715214e-02,   2.63070234e-01,  -2.40345554e-01,\n",
       "         8.26447538e-01,   1.29802763e-01,   1.28322676e+00,\n",
       "         4.47805664e-01,  -5.14270421e+00,  -1.70001983e+00,\n",
       "         5.27915356e-01,  -2.83730588e+00,   1.40994953e-01,\n",
       "         2.04775830e+00,   2.17753795e-01,  -1.22816040e+01,\n",
       "        -2.54884906e-01,   4.09161088e-02,  -2.83805420e-01,\n",
       "        -4.56255918e-01,   8.81344824e-01,  -1.17962842e+00,\n",
       "         1.23663048e-02,  -4.72265069e-01,  -8.32792131e-01,\n",
       "        -7.70820838e-01,  -2.48977030e+00,  -7.78655084e-01,\n",
       "         8.55371995e-01,   2.11620257e+00,  -1.69936681e+00,\n",
       "         1.48775722e+00,  -3.47343928e-01,   1.27717711e+00,\n",
       "        -1.06756726e+00,   2.01664679e+00,   2.24939895e+00,\n",
       "        -1.68419339e+01,  -1.85092331e+01,  -3.61971962e+00,\n",
       "        -2.70331654e-01,   1.04552685e+00,   4.86961028e+00,\n",
       "         1.38043962e+00,   2.47989739e+00,   2.50446749e+00,\n",
       "         1.67955919e+00,  -4.29896448e-01,   2.74021435e+00,\n",
       "         8.72302002e-01,   4.95098361e+00,   1.90548050e+00,\n",
       "         2.40712247e+00,  -3.22654076e-01,   0.00000000e+00,\n",
       "         2.83547754e+00,   1.21311576e+00,   2.34581076e+00,\n",
       "        -5.91415328e+00,   7.74155445e-01,   2.81811750e+00,\n",
       "        -1.37732139e-01,   2.85522883e+00,   1.26514970e+00,\n",
       "         2.41555150e+00,   2.68557989e+00,   1.78534289e+00,\n",
       "         1.77521409e-01,   9.94762040e+00,  -1.34694276e+00,\n",
       "         2.00635477e+01,   8.48014419e+00,  -1.62975856e+00,\n",
       "         1.79507547e+00,   5.20700371e-01,   2.28850039e+00,\n",
       "        -1.04312325e+01,  -1.81208423e+00,   1.19484394e+00,\n",
       "         1.15471485e+00,   1.27137344e+00,  -4.22591469e+00,\n",
       "         1.59639546e+00,  -1.89486353e+00,   6.48690940e-01,\n",
       "         1.87372873e+00,   6.30852935e-01,  -1.00194229e+00,\n",
       "         2.61004817e+00])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ordered_coefs = sorted(range(len(regr.coef_)), key=lambda k: regr.coef_[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best = ordered_coefs[:117]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1295.82717103])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.predict(to_binary(best,130))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1315.7436556481646, 1150.0549340783111)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_forest_train(df2,14,4, best, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it's good, because with random search, we didn't get something better than 1325\n",
    "It's a idea worth exploring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-39a8bfbdb6e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mLOGGER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Prediction of new cv:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mLOGGER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_binary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m130\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mcv_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrand_forest_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mLOGGER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_err\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"   \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_err\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mnew_entry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv_err\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_err\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-3b41163b4e01>\u001b[0m in \u001b[0;36mrand_forest_train\u001b[0;34m(df2, depth, nb_trees, features, sample_data, verbose)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[1;31m# Run cross-validation, and choose the best set of parameters.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mcvModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrossval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mcv_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcvModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavgMetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mtrain_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyRegressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcvModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32mC:\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\ml\\tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[1;31m# TODO: duplicate evaluator to take extra params from input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0mmetric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meva\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnFolds\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\ml\\evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\ml\\evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \"\"\"\n\u001b[1;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.3-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32mC:\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.3-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.3-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    key = \"depth 14 trees 4 features 117\"\n",
    "    x , y = combine(dic, dic1, key)\n",
    "    LOGGER.warn(\"number of entries: \")\n",
    "    LOGGER.warn(len(y))\n",
    "    regr = linear_model.Ridge(alpha=0.01)\n",
    "    LOGGER.warn(\"min=\" + str(np.min(y)))\n",
    "    LOGGER.warn(\"CV=\")\n",
    "    LOGGER.warn(str(-np.mean(cross_val_score(regr, x, y, scoring='neg_mean_squared_error', cv = len(y)))))\n",
    "    regr.fit(x,y)\n",
    "    ordered_coefs = sorted(range(len(regr.coef_)), key=lambda k: regr.coef_[k])\n",
    "    best = sorted(ordered_coefs[:122-5])\n",
    "    \n",
    "    # Take at random if it was already tried\n",
    "    while to_binary(best,130) in x:\n",
    "        LOGGER.warn(\"Already done\")\n",
    "        best = sorted(ordered_coefs[8+5:])\n",
    "        if to_binary(best,130) in x:\n",
    "            best = delete_random(features_to_keep + list(range(116,130)),5)\n",
    "        else:\n",
    "            LOGGER.warn(\"trying worst case\")\n",
    "    \n",
    "    removed = [a for a in list(range(130)) if a not in best]\n",
    "    LOGGER.warn(\"Removed:\")\n",
    "    LOGGER.warn(str(removed))\n",
    "    LOGGER.warn(\"Prediction of new cv:\")\n",
    "    LOGGER.warn(str(regr.predict(to_binary(best,130))))\n",
    "    cv_err, train_err = rand_forest_train(df2,14,4, best, verbose=False)\n",
    "    LOGGER.warn(str(cv_err) + \"   \" + str(train_err))\n",
    "    new_entry = tuple([best,cv_err,train_err])\n",
    "    dic[key].append(new_entry)\n",
    "    dic.sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.01, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x , y = combine(dic, dic1, key)\n",
    "regr = linear_model.Ridge(alpha=0.01)\n",
    "regr.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_idx(list_):\n",
    "    return [i for i in range(130) if list_[i]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_test_csv():\n",
    "    # Load the test file, create a new submission file and print it's name.\n",
    "    \n",
    "    input_path = \"test.csv\"\n",
    "    raw_data = sc.textFile(input_path)\n",
    "\n",
    "    # extract the header\n",
    "    header = raw_data.first()\n",
    "\n",
    "    # replace invalid data with NULL and remove header\n",
    "    cleaned_data = raw_data.filter(lambda row: row != header)\n",
    "    \n",
    "    cleaned_data_splitted_test = cleaned_data.map(lambda x: to_tuple(x, \",\"))\n",
    "    \n",
    "    final_rdd_test = cleaned_data_splitted_test.map(replace)\n",
    "    df_test = sqlContext.createDataFrame(final_rdd_test.map(lambda x: (x[0], Vectors.dense(x[1:117]), Vectors.dense(x[117:]))), [\"id\", \"cat_features\", \"cont_features\"])\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1_test = customTransformer(\"cont_features\", \"cont_bucked_features\", 7).transform(load_test_csv(), t_bucketize)\n",
    "df2_test = apply(df1_test,[indexer_cat, indexer_cont])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x,y = combine(dic, dic1, \"depth 14 trees 4 features 117\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = np.argmin(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1309.7046554173232"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_features = to_idx(x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth: 14\n",
      "1312.9073293278016\n",
      "1151.7838126972713\n",
      "[VectorSlicer_453792205a9c6e832430, VectorSlicer_46449dd1c31f2de920c6, VectorAssembler_4d45b95e11460b32f914, RandomForestRegressionModel (uid=rfr_6a797b066cb0) with 4 trees]\n"
     ]
    }
   ],
   "source": [
    "df_with_predictions = rand_forest_train(df2,14,4, best_features, test_set=df2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125546"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_predictions.rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_test_csv(df_with_predictions):\n",
    "    rdd = df_with_predictions.select([\"id\",\"prediction\"]).rdd\n",
    "    print(rdd.first())\n",
    "    \n",
    "    rdd = rdd.map(lambda x: (x[0],x[1]))\n",
    "    \n",
    "    submission_list = rdd.collect()\n",
    "    \n",
    "    submission_number = 0\n",
    "    while os.path.isfile(\"submission_\" + str(submission_number) + \".csv\"):\n",
    "        submission_number += 1\n",
    "    \n",
    "    \n",
    "    f = open(\"submission_\" + str(submission_number) + \".csv\", 'w')\n",
    "    f.write(\"id,loss\\n\")\n",
    "    for tup in submission_list:\n",
    "        f.write(str(tup[0]) + \",\" + str(tup[1]) + \"\\n\")\n",
    "    f.close()\n",
    "    \n",
    "    print(\"Submission file number \" + str(submission_number) +\" was created\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=4, prediction=2395.1693133909216)\n",
      "Submission file number 11 was created\n"
     ]
    }
   ],
   "source": [
    "write_test_csv(df_with_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
