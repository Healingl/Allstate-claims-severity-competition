{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported Spark Modules\n",
      "5\n",
      "defaultdict(<class 'int'>, {'akka': 1, 'hadoop': 1, 'scala': 1, 'spark': 1, 'java': 1})\n"
     ]
    }
   ],
   "source": [
    "#### This cell is to make spark work on a windows laptop\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Path for spark source folder\n",
    "os.environ['SPARK_HOME']=\"C:\\spark-2.0.1-bin-hadoop2.7\"\n",
    "\n",
    "# Append pyspark  to Python Path\n",
    "sys.path.append(\"C:\\spark-2.0.1-bin-hadoop2.7\\python\")\n",
    "sys.path.append(\"C:\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.3-src.zip\")\n",
    "#os.environ['SPARK_EXECUTOR_MEMORY']=\"5G\"\n",
    "\n",
    "try:\n",
    "    from pyspark import SparkContext\n",
    "    from pyspark import SparkConf\n",
    "    print (\"Successfully imported Spark Modules\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print (\"Can not import Spark Modules\", e)\n",
    "    sys.exit(1)\n",
    "\n",
    "# Initialize SparkContext\n",
    "sc = SparkContext()\n",
    "words = sc.parallelize([\"scala\",\"java\",\"hadoop\",\"spark\",\"akka\"])\n",
    "print (words.count())\n",
    "print(words.countByValue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from IPython.display import display\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql import SparkSession\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as func\n",
    "import matplotlib.patches as mpatches\n",
    "import time as time\n",
    "from matplotlib.patches import Rectangle\n",
    "import datetime\n",
    "import ast\n",
    "from operator import add\n",
    "import math\n",
    "from itertools import combinations\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import NaiveBayes\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "log4jLogger = sc._jvm.org.apache.log4j\n",
    "LOGGER = log4jLogger.LogManager.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First step: cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows before cleaning: 188319\n",
      "number of rows after cleaning: 188319\n",
      "Number of partitions: 2\n"
     ]
    }
   ],
   "source": [
    "input_path = \"train.csv\"\n",
    "raw_data = sc.textFile(input_path)\n",
    "print(\"number of rows before cleaning:\", raw_data.count())\n",
    "\n",
    "# extract the header\n",
    "header = raw_data.first()\n",
    "\n",
    "# replace invalid data with NULL and remove header\n",
    "cleaned_data = raw_data.filter(lambda row: row != header)\n",
    "\n",
    "print(\"number of rows after cleaning:\", raw_data.count())\n",
    "print(\"Number of partitions: \" + str(raw_data.getNumPartitions()))\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to put our data into a dataframe, so that it's going to be easier to plot graphs and get a better insight into our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'id;cat1;cat2;cat3;cat4;cat5;cat6;cat7;cat8;cat9;cat10;cat11;cat12;cat13;cat14;cat15;cat16;cat17;cat18;cat19;cat20;cat21;cat22;cat23;cat24;cat25;cat26;cat27;cat28;cat29;cat30;cat31;cat32;cat33;cat34;cat35;cat36;cat37;cat38;cat39;cat40;cat41;cat42;cat43;cat44;cat45;cat46;cat47;cat48;cat49;cat50;cat51;cat52;cat53;cat54;cat55;cat56;cat57;cat58;cat59;cat60;cat61;cat62;cat63;cat64;cat65;cat66;cat67;cat68;cat69;cat70;cat71;cat72;cat73;cat74;cat75;cat76;cat77;cat78;cat79;cat80;cat81;cat82;cat83;cat84;cat85;cat86;cat87;cat88;cat89;cat90;cat91;cat92;cat93;cat94;cat95;cat96;cat97;cat98;cat99;cat100;cat101;cat102;cat103;cat104;cat105;cat106;cat107;cat108;cat109;cat110;cat111;cat112;cat113;cat114;cat115;cat116;cont1;cont2;cont3;cont4;cont5;cont6;cont7;cont8;cont9;cont10;cont11;cont12;cont13;cont14;loss'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our taget is the loss. The id isn't actually useful and will be removed later. \"cat\" means categorical feature and \"cont\" means a continuous feature. we will use that to create the data schema. Note that we'll convert strings in Ints since it's easier to manipulate them later. (In particular for partitioning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = header.split(\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cats = names[1:117]\n",
    "conts = names[117:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_StructField(string):\n",
    "    hint = string[:3]\n",
    "    if hint == \"cat\":\n",
    "        datatype = types.IntegerType()\n",
    "    elif hint == \"con\":\n",
    "        datatype = types.FloatType()\n",
    "    elif hint == \"id\":\n",
    "        datatype = types.IntegerType()\n",
    "    elif hint == \"los\":\n",
    "        datatype = types.FloatType()\n",
    "    else:\n",
    "        raise ValueError(\"Can\\'t read this string:\" + hint )\n",
    "\n",
    "    return types.StructField(string, datatype, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "structField_list = [create_StructField(string) for string in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_schema = types.StructType(structField_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tryeval(val,column_number):\n",
    "    if column_number == 0:\n",
    "        return int(val)\n",
    "    elif 1 <= column_number <= 116:\n",
    "        return val\n",
    "    elif 117 <= column_number <= 131:\n",
    "        return float(val)\n",
    "    else:\n",
    "        raise Exception(\"There is a big problem\")\n",
    "\n",
    "def to_tuple(string, character = \";\"):\n",
    "    list_of_strings = string.split(character)\n",
    "    return tuple(tryeval(string, n) for n, string in enumerate(list_of_strings))\n",
    "\n",
    "cleaned_data_splitted = cleaned_data.map(lambda x: to_tuple(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188318"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data_splitted.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll try to convert the categorical values in integers as it's way easier to work with later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_tuples(list_):\n",
    "    return tuple((string,) for string in list_)\n",
    "\n",
    "def fusion(x, y):\n",
    "    return tuple(tuple(set(xi + yi)) for xi, yi in zip(x,y))\n",
    "\n",
    "list_of_dictionaries = []\n",
    "a = cleaned_data_splitted.map(lambda x: to_tuples(x[1:117])).reduce(fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_tuples = tuple(tuple(sorted(tup)) for tup in a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the categories in order. We'll put them in a list of dictionaries as it makes it simpler to modify the RDD after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tup in sorted_tuples:\n",
    "    my_dict = dict()\n",
    "    for idx, cat in enumerate(tup):\n",
    "        my_dict[cat] = idx\n",
    "    list_of_dictionaries.append(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0, 'B': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_dictionaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bListOfDictionaries = sc.broadcast(list_of_dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace(row):\n",
    "    strings = row[1:117]\n",
    "    my_dicts = bListOfDictionaries.value\n",
    "    tuple_of_ints = ()\n",
    "    for dict_, string in zip(my_dicts, strings):\n",
    "        try:\n",
    "            tuple_of_ints += (dict_[string],)\n",
    "        except KeyError:\n",
    "            tuple_of_ints += (0,)\n",
    "    return (row[0],) + tuple_of_ints + row[117:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_rdd = cleaned_data_splitted.map(replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we now have integers instead of \"A\" or \"C\" or any other string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 1, 1, 3, 3, 1, 3, 2, 1, 3, 1, 0, 0, 0, 0, 0, 3, 1, 2, 4, 0, 2, 15, 1, 6, 0, 0, 8, 4, 6, 9, 6, 45, 28, 2, 19, 55, 0, 14, 269, 0.7263, 0.245921, 0.187583, 0.789639, 0.310061, 0.718367, 0.33506, 0.3026, 0.67135, 0.8351, 0.569745, 0.594646, 0.822493, 0.714843, 2213.18)\n"
     ]
    }
   ],
   "source": [
    "print(final_rdd.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_delete = [2,3,4,5,6,7,8,96]    # cela correspond à cat2, cat3, cat4 ...\n",
    "features_to_keep = list(range(116))\n",
    "for idx in to_delete:\n",
    "    features_to_keep.remove(idx - 1) # Car to_delete commence à 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame(final_rdd.map(lambda x: (float(x[-1]), Vectors.dense(x[1:-1]))), [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=20).fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(train, test) = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(labelCol=\"label\", featuresCol=\"indexedFeatures\", maxDepth=20, maxBins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[indexer, dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictionsTrain = model.transform(train)\n",
    "predictionsTest = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on test data = 631.863\n",
      "Mean Squared Error (MSE) on test data = 1570.83\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(metricName=\"mae\")\n",
    "rmseTrain = evaluator.evaluate(predictionsTrain)\n",
    "print(\"Mean Squared Error (MSE) on test data = %g\" % rmseTrain)\n",
    "\n",
    "rmseTest = evaluator.evaluate(predictionsTest)\n",
    "print(\"Mean Squared Error (MSE) on test data = %g\" % rmseTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining the transformations\n",
    "slicer = VectorSlicer(inputCol=\"features\", outputCol=\"featuresSliced\", indices = features_to_keep)\n",
    "indexer = VectorIndexer(inputCol=\"featuresSliced\", outputCol=\"indexedFeatures\", maxCategories=20).fit(slicer.transform(df))\n",
    "dt = DecisionTreeRegressor(labelCol=\"label\", featuresCol=\"indexedFeatures\", maxBins=20, minInstancesPerNode=1, minInfoGain=0.0, maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, impurity=\"variance\")\n",
    "\n",
    "# defining the pipeline\n",
    "pipeline = Pipeline(stages=[slicer, indexer, dt])\n",
    "\n",
    "# defining the parameters to test\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [5]) \\\n",
    "    .build()\n",
    "    \n",
    "myRegressor = RegressionEvaluator(metricName=\"mae\")\n",
    "    \n",
    "# defining the cross-validation\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=myRegressor,\n",
    "                          numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation results (we see that the depth of 10 is better):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1433.5199130594435]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_4377bc1d5dd4a225144b"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training error for the best model (we're overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1424.2979358651687"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RegressionEvaluator(metricName=\"mae\").evaluate(cvModel.bestModel.transform(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try with all the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=20).fit(df)\n",
    "dt = DecisionTreeRegressor(labelCol=\"label\", featuresCol=\"indexedFeatures\", maxBins=20)\n",
    "\n",
    "# defining the pipeline\n",
    "pipeline = Pipeline(stages=[slicer, indexer, dt])\n",
    "\n",
    "# defining the parameters to test\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [10, 20]) \\\n",
    "    .build()\n",
    "    \n",
    "# defining the cross-validation\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(metricName=\"mae\"),\n",
    "                          numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it better than last time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1359.6923954368876, 1588.2854298996053]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that those features bring absolutely no information. As shown in the other notebook \"Allstate competition.ipynb\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|  label|            features|      scaledFeatures|\n",
      "+-------+--------------------+--------------------+\n",
      "|2213.18|[0.0,1.0,0.0,1.0,...|[0.0,1.0,0.0,1.0,...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "None\n",
      "[1331.6055414944035]\n",
      "1329.4492453430448\n",
      "[MinMaxScaler_4aeb955f79fe5cfac729, LinearRegression_48a4bb18db05c6db6cc5]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "# Defining the transformations\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\").fit(df)\n",
    "\n",
    "print(scaler.transform(df).show(1))\n",
    "dt = LinearRegression(featuresCol=\"scaledFeatures\")\n",
    "\n",
    "# defining the pipeline\n",
    "pipeline = Pipeline(stages=[scaler,dt])\n",
    "\n",
    "# defining the parameters to test\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxIter, [25]) \\\n",
    "    .build()\n",
    "    \n",
    "myEvaluator = RegressionEvaluator(metricName=\"mae\")\n",
    "# defining the cross-validation\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=myEvaluator,\n",
    "                          numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(df)\n",
    "\n",
    "print(cvModel.avgMetrics)\n",
    "print(myEvaluator.evaluate(cvModel.bestModel.transform(df)))\n",
    "print(cvModel.bestModel.stages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame(final_rdd.map(lambda x: (float(x[-1]), Vectors.dense(x[1:117]), Vectors.dense(x[117:-1]))), [\"label\", \"cat_features\", \"cont_features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|  label|        cat_features|       cont_features|\n",
      "+-------+--------------------+--------------------+\n",
      "|2213.18|[0.0,1.0,0.0,1.0,...|[0.7263,0.245921,...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try different configurations for a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining the transformations\n",
    "indexer = VectorIndexer(inputCol=\"cat_features\", outputCol=\"indexedFeatures\", maxCategories=20).fit(df)\n",
    "slicer_cat = VectorSlicer(inputCol=\"indexedFeatures\", outputCol=\"sliced_cat_Features\", indices=features_to_keep)\n",
    "slicer_cont = VectorSlicer(inputCol=\"cont_features\", outputCol=\"sliced_cont_Features\", indices = list(range(1)))\n",
    "assembler = VectorAssembler(inputCols=[\"sliced_cat_Features\", \"sliced_cont_Features\"], outputCol=\"features\")\n",
    "\n",
    "rf = RandomForestRegressor(labelCol=\"label\", featuresCol=\"features\", maxBins=280,\\\n",
    "                           maxMemoryInMB=500, subsamplingRate=0.9, cacheNodeIds=True, \n",
    "                           checkpointInterval=10, numTrees=5)\n",
    "\n",
    "# defining the pipeline\n",
    "pipeline = Pipeline(stages=[indexer, slicer_cat, slicer_cont, assembler, rf])\n",
    "\n",
    "# defining the parameters to test\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [3]) \\\n",
    "    .build()\n",
    "    \n",
    "myRegressor = RegressionEvaluator(metricName=\"mae\")\n",
    "    \n",
    "# defining the cross-validation\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=myRegressor,\n",
    "                          numFolds=3)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1506.7323171092319]\n",
      "1505.6181538122055\n",
      "[VectorIndexer_4557be16f2cf9e01e187, VectorSlicer_4ee8b70f28ee273d0226, VectorSlicer_4740827b3bfdca001282, VectorAssembler_4a18a2d0732f223883b7, RandomForestRegressionModel (uid=rfr_a9b44af90c4d) with 5 trees]\n"
     ]
    }
   ],
   "source": [
    "print(cvModel.avgMetrics)\n",
    "print(myRegressor.evaluate(cvModel.bestModel.transform(df)))\n",
    "print(cvModel.bestModel.stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label', 'cat_features', 'cont_features']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(label=2213.18, cat_features=DenseVector([0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 3.0, 1.0, 1.0, 3.0, 3.0, 1.0, 3.0, 2.0, 1.0, 3.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 1.0, 2.0, 4.0, 0.0, 2.0, 15.0, 1.0, 6.0, 0.0, 0.0, 8.0, 4.0, 6.0, 9.0, 6.0, 45.0, 28.0, 2.0, 19.0, 55.0, 0.0, 14.0, 269.0]), cont_features=DenseVector([0.7263, 0.2459, 0.1876, 0.7896, 0.3101, 0.7184, 0.3351, 0.3026, 0.6714, 0.8351, 0.5697, 0.5946, 0.8225, 0.7148]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def t(element, fitInfo = 0, args = []):\n",
    "        new_tup = tuple(int(scalar*args[0]) for scalar in element)\n",
    "        return Vectors.dense(new_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class customTransformer:\n",
    "    \n",
    "    def __init__(self, inputCol, outputCol, *others):\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "        self.args = list(others)\n",
    "        self.fitInfo = 0\n",
    "        \n",
    "    # Store information taken from the dataframe\n",
    "    def fit(self, df):\n",
    "        idx = df.columns.index(self.inputCol)\n",
    "        self.fitInfo = fitting(df.rdd.map(lambda x: x[idx]))\n",
    "        return self\n",
    "            \n",
    "    # This is the function to implement\n",
    "    def fitting(self, rdd):\n",
    "        return 0\n",
    "            \n",
    "    # This transforms a dataframe into another dataframe\n",
    "    def transform(self, df):\n",
    "        names = df.columns\n",
    "        idx = names.index(self.inputCol)\n",
    "        bInfo = sc.broadcast([0, self.args])\n",
    "        new_column = df.rdd.map(lambda x: t(x[idx], bInfo.value[0], bInfo.value[1]), preservesPartitioning = True)\n",
    "        print(new_column.first())\n",
    "        old_rdd = df.rdd.map(lambda x: list(x))\n",
    "        new_rdd = old_rdd.zip(new_column).map(lambda x: x[0] + [x[1]])\n",
    "        print(new_rdd.first())\n",
    "        print(self.outputCol)\n",
    "        print(names)\n",
    "        new_names = names + [self.outputCol]\n",
    "        print(new_names)\n",
    "        return sqlContext.createDataFrame(new_rdd, new_names)\n",
    "            \n",
    "    # This is a function to implement, \n",
    "    # fitInfo is the information gotten from the fit function\n",
    "    # args are the arguments given when creating the instance\n",
    "    @staticmethod\n",
    "    def transforming(element, fitInfo = 0, args = []):\n",
    "        return element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 8, 6, DenseVector([5.0, 3.0, 7.0])]\n"
     ]
    }
   ],
   "source": [
    "print([5,8,6] + [Vectors.dense([5,3,7])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply(df, listOfTransformers):\n",
    "    df1 = df\n",
    "    for transformer in listOfTransformers:\n",
    "        df1 = transformer.transform(df1)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class vectorBucketizer(customTransformer):\n",
    "    \n",
    "    def __init__(self, inputCol, outputCol, *others):\n",
    "        customTransformer.__init__(self, inputCol, outputCol, *others)\n",
    "    \n",
    "    @staticmethod\n",
    "    def transforming(element, fitInfo = 0, args = []):\n",
    "        new_tup = tuple(int(scalar*args) for scalar in element)\n",
    "        return Vectors.dense(new_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.0,2.0,1.0,7.0,3.0,7.0,3.0,3.0,6.0,8.0,5.0,5.0,8.0,7.0]\n",
      "[2213.18, DenseVector([0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 3.0, 1.0, 1.0, 3.0, 3.0, 1.0, 3.0, 2.0, 1.0, 3.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 1.0, 2.0, 4.0, 0.0, 2.0, 15.0, 1.0, 6.0, 0.0, 0.0, 8.0, 4.0, 6.0, 9.0, 6.0, 45.0, 28.0, 2.0, 19.0, 55.0, 0.0, 14.0, 269.0]), DenseVector([0.7263, 0.2459, 0.1876, 0.7896, 0.3101, 0.7184, 0.3351, 0.3026, 0.6714, 0.8351, 0.5697, 0.5946, 0.8225, 0.7148]), DenseVector([7.0, 2.0, 1.0, 7.0, 3.0, 7.0, 3.0, 3.0, 6.0, 8.0, 5.0, 5.0, 8.0, 7.0])]\n",
      "bucked_features\n",
      "['label', 'cat_features', 'cont_features']\n",
      "['label', 'cat_features', 'cont_features', 'bucked_features']\n"
     ]
    }
   ],
   "source": [
    "df1 = vectorBucketizer(\"cont_features\", \"bucked_features\", 10).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(label=2213.18, cat_features=DenseVector([0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 3.0, 1.0, 1.0, 3.0, 3.0, 1.0, 3.0, 2.0, 1.0, 3.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 1.0, 2.0, 4.0, 0.0, 2.0, 15.0, 1.0, 6.0, 0.0, 0.0, 8.0, 4.0, 6.0, 9.0, 6.0, 45.0, 28.0, 2.0, 19.0, 55.0, 0.0, 14.0, 269.0]), cont_features=DenseVector([0.7263, 0.2459, 0.1876, 0.7896, 0.3101, 0.7184, 0.3351, 0.3026, 0.6714, 0.8351, 0.5697, 0.5946, 0.8225, 0.7148]), bucked_features=DenseVector([7.0, 2.0, 1.0, 7.0, 3.0, 7.0, 3.0, 3.0, 6.0, 8.0, 5.0, 5.0, 8.0, 7.0]))"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
